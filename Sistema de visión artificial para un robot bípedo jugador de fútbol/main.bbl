\begin{thebibliography}{1}

\bibitem{guo2022attention}
M.-H. Guo, T.-X. Xu, J.-J. Liu, Z.-N. Liu, P.-T. Jiang, T.-J. Mu, S.-H. Zhang,
  R.~R. Martin, M.-M. Cheng, and S.-M. Hu.
\newblock Attention mechanisms in computer vision: A survey.
\newblock {\em Computational visual media}, 8(3):331--368, 2022.

\bibitem{khan2022transformers}
S.~Khan, M.~Naseer, M.~Hayat, S.~W. Zamir, F.~S. Khan, and M.~Shah.
\newblock Transformers in vision: A survey.
\newblock {\em ACM computing surveys (CSUR)}, 54(10s):1--41, 2022.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\end{thebibliography}
