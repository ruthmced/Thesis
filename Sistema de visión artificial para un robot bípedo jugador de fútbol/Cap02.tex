\chapter{Theoretical Framework}\label{sec:theoreticalframework}
%se presenta, se discute, se revisa, se dedica a, tal tiene la intención de recordar conceptos, 
\section{Transformers}
A \textit{Transformer} is defined as a simple network architecture \cite{vaswani2017attention} based on attention mechanisms for diverting attention to the most important regions of an image and disregarding irrelevant parts \cite{guo2022attention} and have a canonical implementation called multi-head attention which is optimized for parallelization. \cite{khan2022transformers} Due to their simple design, Transformers require minimal inductive biases and are naturally suited as set-functions. Also allows processing multiple modalities (images, videos, text, speech). 
Unlike models that use convolutional neural networks as basic building block,in Transformer, the number of operations required to relate signals from two arbitrary input or output positions is reduced to a constant number of operations. \cite{vaswani2017attention}

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
%		\includegraphics[width=\textwidth]{}
		\caption{}
		\label{fig:image1}
	\end{subfigure}

\end{figure}

\section{} 
%Un párrafo para cada espaico de color 
